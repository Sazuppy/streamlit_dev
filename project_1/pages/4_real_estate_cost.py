from pyspark.sql import SparkSession
import streamlit as st
import numpy as np
import pandas as pd
import dill
import pyspark

from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark import SparkConf
from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder
from pyspark.ml.regression import LinearRegression

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ Spark
spark = SparkSession.builder.appName("example").getOrCreate()

st.set_page_config(page_title="# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∂–∏–ª—å—è", page_icon="üìà")

st.markdown('# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∂–∏–ª—å—è')

st.write(
    """–í –ø—Ä–æ–µ–∫—Ç–µ –≤–∞–º –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –æ –∂–∏–ª—å–µ –≤ –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–∏ –≤ 1990 –≥–æ–¥—É. –ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –º–µ–¥–∏–∞–Ω–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å –¥–æ–º–∞ –≤ –∂–∏–ª–æ–º –º–∞—Å—Å–∏–≤–µ. 
    –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∏ —Å–¥–µ–ª–∞–π—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ RMSE, MAE –∏ R2.

–í –∫–æ–ª–æ–Ω–∫–∞—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ:  
- longitude ‚Äî —à–∏—Ä–æ—Ç–∞;
- latitude ‚Äî –¥–æ–ª–≥–æ—Ç–∞;
- housing_median_age ‚Äî –º–µ–¥–∏–∞–Ω–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç –∂–∏—Ç–µ–ª–µ–π –∂–∏–ª–æ–≥–æ –º–∞—Å—Å–∏–≤–∞;
- total_rooms ‚Äî –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç –≤ –¥–æ–º–∞—Ö –∂–∏–ª–æ–≥–æ –º–∞—Å—Å–∏–≤–∞;
- total_bedrooms ‚Äî –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∞–ª–µ–Ω –≤ –¥–æ–º–∞—Ö –∂–∏–ª–æ–≥–æ –º–∞—Å—Å–∏–≤–∞;
- population ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–∂–∏–≤–∞—é—Ç –≤ –∂–∏–ª–æ–º –º–∞—Å—Å–∏–≤–µ;
- households ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–º–æ–≤–ª–∞–¥–µ–Ω–∏–π –≤ –∂–∏–ª–æ–º –º–∞—Å—Å–∏–≤–µ;
- median_income ‚Äî –º–µ–¥–∏–∞–Ω–Ω—ã–π –¥–æ—Ö–æ–¥ –∂–∏—Ç–µ–ª–µ–π –∂–∏–ª–æ–≥–æ –º–∞—Å—Å–∏–≤–∞;
- median_house_value ‚Äî –º–µ–¥–∏–∞–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –¥–æ–º–∞ –≤ –∂–∏–ª–æ–º –º–∞—Å—Å–∏–≤–µ;
- ocean_proximity ‚Äî –±–ª–∏–∑–æ—Å—Ç—å –∫ –æ–∫–µ–∞–Ω—É.
    """
)

st.sidebar.header("–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")

def changes(df):
    pass

def user_input_features():
    longitude = st.sidebar.slider('—à–∏—Ä–æ—Ç–∞', -124.35, -114.31, -120.0)
    latitude = st.sidebar.slider('–¥–æ–ª–≥–æ—Ç–∞', 32.54, 41.9, 35.0)
    housing_median_age = st.sidebar.slider('–º–µ–¥–∏–∞–Ω–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç –∂–∏—Ç–µ–ª–µ–π –∂–∏–ª–æ–≥–æ –º–∞—Å—Å–∏–≤–∞', 0, 70, 20)
    total_rooms = st.sidebar.slider('–æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç –≤ –¥–æ–º–∞—Ö –∂–∏–ª–æ–≥–æ –º–∞—Å—Å–∏–≤–∞', 0, 40000, 2000)
    total_bedrooms = st.sidebar.slider('–æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∞–ª–µ–Ω –≤ –¥–æ–º–∞—Ö –∂–∏–ª–æ–≥–æ –º–∞—Å—Å–∏–≤–∞', 0, 7000, 2000)
    population = st.sidebar.slider('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–∂–∏–≤–∞—é—Ç –≤ –∂–∏–ª–æ–º –º–∞—Å—Å–∏–≤–µ', 0, 40000, 5000)
    households = st.sidebar.slider('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–º–æ–≤–ª–∞–¥–µ–Ω–∏–π –≤ –∂–∏–ª–æ–º –º–∞—Å—Å–∏–≤–µ', 1, 7000, 400)
    median_income = st.sidebar.slider('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∑—Ä–æ—Å–ª—ã—Ö –ø–æ—Å—Ç–æ—è–ª—å—Ü–µ–≤', 0, 5, 40)
    ocean_proximity = st.sidebar.selectbox('–±–ª–∏–∑–æ—Å—Ç—å –∫ –æ–∫–µ–∞–Ω—É', ('NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND'))
       
    data = {'longitude': longitude,
            'latitude': latitude,
            'housing_median_age': housing_median_age,
            'total_rooms': total_rooms,
            'total_bedrooms': total_bedrooms,
            'population': population,
            'households': households,
            'median_income': median_income,
            'ocean_proximity': ocean_proximity
            }
    features = pd.DataFrame(data, index=[0])
    return features

df = user_input_features()


st.subheader('–¢–∞–±–ª–∏—Ü–∞ —Å –≤–≤–µ–¥–µ–Ω–Ω—ã–º–∏ –≤–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:')
st.write(df)

st.map(df, latitude='latitude', longitude='longitude', zoom=3)
df = spark.createDataFrame(df)
    
def preprocessing_data(df, scaler, ohe, idx):
    categorical_cols = ['ocean_proximity']
    numerical_cols  = ['longitude', 'latitude', 'housing_median_age','total_rooms','total_bedrooms','population','households', 'median_income']
    df = idx.transform(df)
    df = ohe.transform(df)
    categorical_assembler = VectorAssembler(inputCols=[i + '_ohe' for i in categorical_cols], outputCol='categorical_features')
    df = categorical_assembler.transform(df)
    numerical_assembler = VectorAssembler(inputCols=numerical_cols, outputCol="numerical_features")
    df = numerical_assembler.transform(df)
    df = scaler.transform(df)
    all_features = ['categorical_features','numerical_features_scaled']
    final_assembler = VectorAssembler(inputCols=all_features, outputCol='features')
    df = final_assembler.transform(df)
            
    return df
    
@st.cache_resource
def get_model():
    with open('models/model_real_estate_cost.dill', 'rb') as f:
        model = dill.load(f)
    with open('models/scaler_real_estate_cost.dill', 'rb') as f:
        sc_model = dill.load(f)
    with open('models/ohe_real_estate_cost.dill', 'rb') as f:
        ohe_model = dill.load(f)
    with open('models/idx_real_estate_cost.dill', 'rb') as f:
        idx_model = dill.load(f)
    return model, sc_model, ohe_model, idx_model

model, sc_model, ohe_model, idx_model = get_model()

df_new = preprocessing_data(df, sc_model, ohe_model, idx_model)

# prediction = model.predict(df_new)
# prediction_proba = model.predict_proba(df_new)


# st.subheader('–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å')
# rounded_prediction = np.around(prediction)
# st.write(str(abs(rounded_prediction.item())) + ' $')