import streamlit as st
import pandas as pd
import pickle
import transformers as tfs
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np
import torch as t
from transformers import AutoTokenizer, AutoModelForSequenceClassification

st.set_page_config(page_title="# –í—ã—è–≤–ª–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Å BERT", page_icon="üìà")

st.markdown('# –í—ã—è–≤–ª–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Å BERT')

with st.expander("–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"):
    st.write("""
        –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω ¬´–í–∏–∫–∏—à–æ–ø¬ª –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å. 
        –¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤, –∫–∞–∫ –≤ –≤–∏–∫–∏-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö. 
        –¢–æ –µ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö. 
        –ú–∞–≥–∞–∑–∏–Ω—É –Ω—É–∂–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∏—Ö –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é. 
    """)


def detect_language(text):
    first_letter = text[0].lower()
    if '–∞' <= first_letter <= '—è':
        return 'ru'
    
    
def ru_bert_comments(text):

    model_checkpoint = 'cointegrated/rubert-tiny-toxicity'
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)
    if t.cuda.is_available():
        model.cuda()
        
    def text2toxicity(text, aggregate=True):
        """ Calculate toxicity of a text (if aggregate=True) or a vector of toxicity aspects (if aggregate=False)"""
        with t.no_grad():
            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)
            proba = t.sigmoid(model(**inputs).logits).cpu().numpy()
        if isinstance(text, str):
            proba = proba[0]
        if aggregate:
            return 1 - proba.T[0] * (1 - proba.T[-1])
        return proba
   
    return round(text2toxicity(text, True))

def preprocessing(text):
    lang = detect_language(text)
    if lang == "ru":
        return ru_bert_comments(str(text))
    else:
        tokenizer = tfs.AutoTokenizer.from_pretrained('unitary/toxic-bert')
        model = tfs.AutoModel.from_pretrained('unitary/toxic-bert')
    tokenized = tokenizer.encode(text, add_special_tokens=True)
    
    if len(tokenized) > 512:
        truncated_tokens = tokenized[:510]  
        truncated_tokens = [101] + truncated_tokens + [102]
        tokenized = truncated_tokens
    
    padded = np.array(tokenized + [0] * (512 - len(tokenized)))
    attention_mask = np.where(padded != 0, 1, 0)
    input_ids = t.tensor(padded)
    attention_mask = t.tensor(attention_mask)

    with t.no_grad():
        embeddings = model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))[0][:, 0, :].cpu().numpy()

    return embeddings

def query(features):
    model = pickle.load(open('project_1/models/toxic_comments_bert.pkl', 'rb'))
    predict = model.predict(features)
    return predict

comment = st.text_area("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–∞—Ö", "")
result = None

if comment:
    st.markdown('## –†–µ–∑—É–ª—å—Ç–∞—Ç:')
    embeddings = preprocessing(comment)
    if isinstance(embeddings, int):
        if embeddings == 0:
            result = '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–µ —Ç–æ–∫—Å–∏—á–Ω—ã–π'
        else:
            result = '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —è–≤–ª—è–µ—Ç—Å—è —Ç–æ–∫—Å–∏—á–Ω—ã–º'
    else:        
        if query(embeddings) == 0:
            result = '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–µ —Ç–æ–∫—Å–∏—á–Ω—ã–π'
        else:
            result = '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —è–≤–ª—è–µ—Ç—Å—è —Ç–æ–∫—Å–∏—á–Ω—ã–º'
        
if result is not None:
    st.write(result)
    
    
    
        




